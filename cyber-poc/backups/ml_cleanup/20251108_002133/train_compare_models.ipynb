{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4d05f56",
   "metadata": {},
   "source": [
    "# SQL Injection Detection - ML Training Pipeline\n",
    "\n",
    "**Project:** Cyber Security Intelligent Threat Mitigation  \n",
    "**Stage:** ML Training & Model Comparison  \n",
    "**Date:** November 2025\n",
    "\n",
    "## Overview\n",
    "This notebook trains and compares multiple ML models for SQL injection detection:\n",
    "- **Baseline:** Logistic Regression\n",
    "- **Main Models:** Random Forest & XGBoost\n",
    "- **Features:** TF-IDF (char n-grams) + engineered numeric features\n",
    "\n",
    "## Table of Contents\n",
    "1. Environment Setup & Dependencies\n",
    "2. Data Loading & Merging\n",
    "3. Exploratory Data Analysis\n",
    "4. Data Preprocessing & Cleaning\n",
    "5. Feature Engineering\n",
    "6. Model Training & Cross-Validation\n",
    "7. Model Evaluation & Comparison\n",
    "8. Explainability (SHAP, Feature Importance)\n",
    "9. Threshold Tuning\n",
    "10. Model Export & Integration\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09925c90",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Dependencies\n",
    "\n",
    "Install required packages. If running on Google Colab, this cell will detect and mount Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea28fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect if running on Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"âœ“ Running on Google Colab\")\n",
    "    \n",
    "    # Mount Google Drive (optional)\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Install dependencies\n",
    "    !pip install -q scikit-learn xgboost pandas numpy pyarrow shap joblib matplotlib seaborn imbalanced-learn\n",
    "    \n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"âœ“ Running locally\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(f\"âœ“ Random seed set to {RANDOM_SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db82c0a9",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c170c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import hashlib\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning - XGBoost only\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score, \n",
    "    precision_recall_curve, roc_curve, auc,\n",
    "    precision_score, recall_score, f1_score\n",
    ")\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Model persistence\n",
    "import joblib\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully\")\n",
    "print(f\"  - pandas: {pd.__version__}\")\n",
    "print(f\"  - numpy: {np.__version__}\")\n",
    "print(f\"  - scikit-learn: {__import__('sklearn').__version__}\")\n",
    "print(f\"  - xgboost: {__import__('xgboost').__version__}\")\n",
    "print(\"\\nâœ“ Note: This notebook uses XGBoost only for production deployment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5272259f",
   "metadata": {},
   "source": [
    "## 3. Configuration & Paths\n",
    "\n",
    "Define all paths and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8b4806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project paths\n",
    "if IN_COLAB:\n",
    "    # Adjust these paths for Colab (upload data to Drive)\n",
    "    PROJECT_ROOT = Path('/content/drive/MyDrive/cyber-poc')\n",
    "else:\n",
    "    PROJECT_ROOT = Path('../..')  # Assumes notebook is in ml/notebooks/\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "ML_DATA_DIR = PROJECT_ROOT / 'ml' / 'data'\n",
    "ML_MODELS_DIR = PROJECT_ROOT / 'ml' / 'models'\n",
    "ML_REPORTS_DIR = PROJECT_ROOT / 'ml' / 'reports'\n",
    "DELIVERABLES_DIR = PROJECT_ROOT / 'deliverables'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for dir_path in [ML_DATA_DIR, ML_MODELS_DIR, ML_REPORTS_DIR]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Hyperparameters\n",
    "CONFIG = {\n",
    "    'random_seed': RANDOM_SEED,\n",
    "    'test_size': 0.1,\n",
    "    'val_size': 0.1,\n",
    "    'max_duplicates': 3,\n",
    "    \n",
    "    # TF-IDF parameters\n",
    "    'tfidf': {\n",
    "        'max_features': 5000,\n",
    "        'analyzer': 'char_wb',\n",
    "        'ngram_range': (3, 6),\n",
    "        'min_df': 2,\n",
    "        'max_df': 0.95\n",
    "    },\n",
    "    \n",
    "    # Random Forest parameters\n",
    "    'rf_grid': {\n",
    "        'n_estimators': [100, 200, 400],\n",
    "        'max_depth': [None, 20, 40],\n",
    "        'max_features': ['sqrt', 'log2'],\n",
    "        'class_weight': ['balanced']\n",
    "    },\n",
    "    \n",
    "    # XGBoost parameters\n",
    "    'xgb_grid': {\n",
    "        'n_estimators': [100, 300],\n",
    "        'learning_rate': [0.05, 0.1],\n",
    "        'max_depth': [6, 10],\n",
    "        'subsample': [0.7, 1.0],\n",
    "        'tree_method': ['hist']\n",
    "    },\n",
    "    \n",
    "    # Cross-validation\n",
    "    'cv_folds': 5,\n",
    "    \n",
    "    # Primary metric for model selection\n",
    "    'primary_metric': 'f1_malicious'\n",
    "}\n",
    "\n",
    "print(\"âœ“ Configuration loaded\")\n",
    "print(f\"  - Project root: {PROJECT_ROOT}\")\n",
    "print(f\"  - ML data directory: {ML_DATA_DIR}\")\n",
    "print(f\"  - TF-IDF max features: {CONFIG['tfidf']['max_features']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bf3bf7",
   "metadata": {},
   "source": [
    "## 4. Data Loading & Merging\n",
    "\n",
    "Load all available datasets and merge them into a single canonical dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e936333c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_merge_datasets():\n",
    "    \"\"\"Load all available datasets and merge into canonical format\"\"\"\n",
    "    \n",
    "    datasets = []\n",
    "    \n",
    "    # 1. Load data/dataset.csv\n",
    "    dataset_path = DATA_DIR / 'dataset.csv'\n",
    "    if dataset_path.exists():\n",
    "        print(f\"Loading {dataset_path}...\")\n",
    "        df1 = pd.read_csv(dataset_path)\n",
    "        df1['source_file'] = 'dataset.csv'\n",
    "        datasets.append(df1)\n",
    "        print(f\"  âœ“ Loaded {len(df1):,} rows\")\n",
    "    \n",
    "    # 2. Load FINAL_DATASET_FOR_AI_TEAM_v3 (1).csv\n",
    "    final_dataset_path = PROJECT_ROOT / 'FINAL_DATASET_FOR_AI_TEAM_v3 (1).csv'\n",
    "    if final_dataset_path.exists():\n",
    "        print(f\"\\nLoading {final_dataset_path.name}...\")\n",
    "        df2 = pd.read_csv(final_dataset_path)\n",
    "        df2['source_file'] = 'FINAL_DATASET_FOR_AI_TEAM_v3.csv'\n",
    "        datasets.append(df2)\n",
    "        print(f\"  âœ“ Loaded {len(df2):,} rows\")\n",
    "    \n",
    "    if not datasets:\n",
    "        raise FileNotFoundError(\"No datasets found! Please ensure data files exist.\")\n",
    "    \n",
    "    # Merge all datasets\n",
    "    print(f\"\\nMerging {len(datasets)} dataset(s)...\")\n",
    "    df = pd.concat(datasets, ignore_index=True)\n",
    "    \n",
    "    print(f\"âœ“ Total rows after merge: {len(df):,}\")\n",
    "    print(f\"âœ“ Columns: {list(df.columns)}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load data\n",
    "df_raw = load_and_merge_datasets()\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nFirst 3 rows:\")\n",
    "df_raw.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5196ce85",
   "metadata": {},
   "source": [
    "## 5. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47508639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset(df):\n",
    "    \"\"\"Perform EDA on the dataset\"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"DATASET ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Basic info\n",
    "    print(f\"\\nðŸ“Š Shape: {df.shape}\")\n",
    "    print(f\"ðŸ“Š Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Missing values\n",
    "    print(\"\\nðŸ“Š Missing values:\")\n",
    "    missing = df.isnull().sum()\n",
    "    missing = missing[missing > 0].sort_values(ascending=False)\n",
    "    if len(missing) > 0:\n",
    "        for col, count in missing.items():\n",
    "            print(f\"  - {col}: {count:,} ({count/len(df)*100:.2f}%)\")\n",
    "    else:\n",
    "        print(\"  âœ“ No missing values\")\n",
    "    \n",
    "    # Label distribution\n",
    "    if 'is_malicious' in df.columns:\n",
    "        print(\"\\nðŸ“Š Label distribution:\")\n",
    "        label_counts = df['is_malicious'].value_counts()\n",
    "        for label, count in label_counts.items():\n",
    "            pct = count / len(df) * 100\n",
    "            label_name = 'Malicious' if label == 1 else 'Benign'\n",
    "            print(f\"  - {label_name}: {count:,} ({pct:.2f}%)\")\n",
    "        \n",
    "        # Class imbalance ratio\n",
    "        if len(label_counts) == 2:\n",
    "            imbalance_ratio = label_counts.max() / label_counts.min()\n",
    "            print(f\"  - Imbalance ratio: {imbalance_ratio:.2f}:1\")\n",
    "    \n",
    "    # Attack types\n",
    "    if 'attack_type' in df.columns:\n",
    "        print(\"\\nðŸ“Š Attack type distribution:\")\n",
    "        attack_counts = df['attack_type'].value_counts().head(10)\n",
    "        for attack, count in attack_counts.items():\n",
    "            if pd.notna(attack):\n",
    "                print(f\"  - {attack}: {count:,}\")\n",
    "    \n",
    "    # Source files\n",
    "    if 'source_file' in df.columns:\n",
    "        print(\"\\nðŸ“Š Source file distribution:\")\n",
    "        for source, count in df['source_file'].value_counts().items():\n",
    "            print(f\"  - {source}: {count:,}\")\n",
    "    \n",
    "    # Query length statistics\n",
    "    if 'raw_query' in df.columns:\n",
    "        df['query_length'] = df['raw_query'].fillna('').str.len()\n",
    "        print(\"\\nðŸ“Š Query length statistics:\")\n",
    "        print(f\"  - Min: {df['query_length'].min():.0f}\")\n",
    "        print(f\"  - Max: {df['query_length'].max():.0f}\")\n",
    "        print(f\"  - Mean: {df['query_length'].mean():.2f}\")\n",
    "        print(f\"  - Median: {df['query_length'].median():.0f}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Analyze\n",
    "df_raw = analyze_dataset(df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99a5167",
   "metadata": {},
   "source": [
    "## 6. Visualization: Label Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca6a15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot label distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar plot\n",
    "if 'is_malicious' in df_raw.columns:\n",
    "    label_counts = df_raw['is_malicious'].value_counts()\n",
    "    axes[0].bar(['Benign', 'Malicious'], [label_counts.get(0, 0), label_counts.get(1, 0)], \n",
    "                color=['green', 'red'], alpha=0.7)\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].set_title('Label Distribution')\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add counts on bars\n",
    "    for i, v in enumerate([label_counts.get(0, 0), label_counts.get(1, 0)]):\n",
    "        axes[0].text(i, v + len(df_raw)*0.01, f'{v:,}', ha='center', va='bottom')\n",
    "\n",
    "# Pie chart\n",
    "if 'is_malicious' in df_raw.columns:\n",
    "    axes[1].pie([label_counts.get(0, 0), label_counts.get(1, 0)], \n",
    "                labels=['Benign', 'Malicious'],\n",
    "                colors=['green', 'red'],\n",
    "                autopct='%1.1f%%',\n",
    "                startangle=90)\n",
    "    axes[1].set_title('Label Proportion')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(ML_REPORTS_DIR / 'label_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Visualizations saved to ml/reports/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b326e364",
   "metadata": {},
   "source": [
    "## 7. Data Preprocessing & Cleaning\n",
    "\n",
    "Clean and prepare data for training:\n",
    "1. Remove rows with empty raw_query\n",
    "2. Handle duplicates\n",
    "3. Normalize schema\n",
    "4. Remove PII (anonymize IPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061a04e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, max_duplicates=3):\n",
    "    \"\"\"Clean and preprocess the dataset\"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"DATA PREPROCESSING\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    initial_rows = len(df)\n",
    "    print(f\"\\nInitial rows: {initial_rows:,}\")\n",
    "    \n",
    "    # 1. Remove rows with empty raw_query\n",
    "    df = df[df['raw_query'].notna()]\n",
    "    df = df[df['raw_query'].str.strip() != '']\n",
    "    print(f\"After removing empty queries: {len(df):,} ({initial_rows - len(df):,} removed)\")\n",
    "    \n",
    "    # 2. Create query hash for deduplication\n",
    "    df['query_hash'] = df['raw_query'].apply(\n",
    "        lambda x: hashlib.md5(str(x).encode()).hexdigest()\n",
    "    )\n",
    "    \n",
    "    # 3. Handle duplicates - keep max N duplicates\n",
    "    before_dedup = len(df)\n",
    "    duplicate_counts = df['query_hash'].value_counts()\n",
    "    \n",
    "    # For queries that appear > max_duplicates times, sample randomly\n",
    "    keep_indices = []\n",
    "    for query_hash in df['query_hash'].unique():\n",
    "        query_indices = df[df['query_hash'] == query_hash].index\n",
    "        if len(query_indices) <= max_duplicates:\n",
    "            keep_indices.extend(query_indices)\n",
    "        else:\n",
    "            # Sample max_duplicates randomly\n",
    "            sampled = np.random.choice(query_indices, size=max_duplicates, replace=False)\n",
    "            keep_indices.extend(sampled)\n",
    "    \n",
    "    df = df.loc[keep_indices]\n",
    "    print(f\"After deduplication (max {max_duplicates} per query): {len(df):,} ({before_dedup - len(df):,} removed)\")\n",
    "    \n",
    "    # 4. Anonymize IPs (optional - keep first 2 octets)\n",
    "    if 'source_ip' in df.columns:\n",
    "        df['source_ip'] = df['source_ip'].apply(\n",
    "            lambda x: '.'.join(str(x).split('.')[:2]) + '.XXX.XXX' if pd.notna(x) else 'unknown'\n",
    "        )\n",
    "    \n",
    "    # 5. Ensure label column\n",
    "    if 'is_malicious' not in df.columns and 'label' in df.columns:\n",
    "        df['is_malicious'] = df['label']\n",
    "    \n",
    "    # 6. Fill missing attack_type\n",
    "    if 'attack_type' in df.columns:\n",
    "        df['attack_type'] = df['attack_type'].fillna('benign')\n",
    "    \n",
    "    print(f\"\\nâœ“ Final dataset size: {len(df):,} rows\")\n",
    "    print(f\"âœ“ Removed {initial_rows - len(df):,} rows total ({(initial_rows - len(df))/initial_rows*100:.2f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Preprocess\n",
    "df_clean = preprocess_data(df_raw, max_duplicates=CONFIG['max_duplicates'])\n",
    "\n",
    "# Save cleaned dataset\n",
    "df_clean.to_csv(ML_DATA_DIR / 'merged.csv', index=False)\n",
    "df_clean.to_json(ML_DATA_DIR / 'merged.jl', orient='records', lines=True)\n",
    "\n",
    "print(f\"\\nâœ“ Cleaned dataset saved to:\")\n",
    "print(f\"  - {ML_DATA_DIR / 'merged.csv'}\")\n",
    "print(f\"  - {ML_DATA_DIR / 'merged.jl'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f45162",
   "metadata": {},
   "source": [
    "## 8. Train/Val/Test Split\n",
    "\n",
    "Split data into train (80%), validation (10%), and test (10%) sets with stratification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a67086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_splits(df, test_size=0.1, val_size=0.1, random_state=42):\n",
    "    \"\"\"Create stratified train/val/test splits\"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"CREATING DATA SPLITS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # First split: train+val vs test\n",
    "    X = df['raw_query']\n",
    "    y = df['is_malicious']\n",
    "    \n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, stratify=y, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Second split: train vs val\n",
    "    val_size_adjusted = val_size / (1 - test_size)  # Adjust for already removed test set\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=val_size_adjusted, stratify=y_temp, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Split sizes:\")\n",
    "    print(f\"  - Train: {len(X_train):,} ({len(X_train)/len(df)*100:.1f}%)\")\n",
    "    print(f\"  - Val:   {len(X_val):,} ({len(X_val)/len(df)*100:.1f}%)\")\n",
    "    print(f\"  - Test:  {len(X_test):,} ({len(X_test)/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Label distribution in splits:\")\n",
    "    for name, y_split in [('Train', y_train), ('Val', y_val), ('Test', y_test)]:\n",
    "        counts = y_split.value_counts()\n",
    "        benign = counts.get(0, 0)\n",
    "        malicious = counts.get(1, 0)\n",
    "        print(f\"  {name:5s}: Benign={benign:,} ({benign/len(y_split)*100:.1f}%), \"\n",
    "              f\"Malicious={malicious:,} ({malicious/len(y_split)*100:.1f}%)\")\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# Create splits\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = create_splits(\n",
    "    df_clean, \n",
    "    test_size=CONFIG['test_size'],\n",
    "    val_size=CONFIG['val_size'],\n",
    "    random_state=CONFIG['random_seed']\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ Data splits created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46714052",
   "metadata": {},
   "source": [
    "## 9. Feature Engineering\n",
    "\n",
    "Extract two types of features:\n",
    "1. **TF-IDF features** - Character-level n-grams (3-6)\n",
    "2. **Numeric features** - Hand-engineered counts and flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb29f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_numeric_features(queries):\n",
    "    \"\"\"Extract hand-engineered numeric features from queries\"\"\"\n",
    "    \n",
    "    features = pd.DataFrame()\n",
    "    \n",
    "    # SQL keywords to check\n",
    "    sql_keywords = ['UNION', 'SELECT', 'INSERT', 'UPDATE', 'DELETE', 'DROP', \n",
    "                    'CREATE', 'ALTER', 'EXEC', 'EXECUTE', 'SLEEP', 'WAITFOR',\n",
    "                    'BENCHMARK', 'INFORMATION_SCHEMA', 'XP_CMDSHELL', 'LOAD_FILE']\n",
    "    \n",
    "    # Suspicious characters\n",
    "    susp_chars = [\"'\", '\"', '--', ';', '/*', '*/', '#']\n",
    "    \n",
    "    for query in queries:\n",
    "        query_str = str(query).upper()\n",
    "        \n",
    "        feature_dict = {\n",
    "            'len_raw': len(query_str),\n",
    "            'count_single_quote': query_str.count(\"'\"),\n",
    "            'count_double_quote': query_str.count('\"'),\n",
    "            'count_dashes': query_str.count('--'),\n",
    "            'count_semicolon': query_str.count(';'),\n",
    "            'count_comment': query_str.count('/*') + query_str.count('*/')\n",
    " + query_str.count('#'),\n",
    "            'count_susp_chars': sum(query_str.count(c) for c in susp_chars),\n",
    "            'num_sql_keywords': sum(1 for kw in sql_keywords if kw in query_str),\n",
    "            'has_union': int('UNION' in query_str),\n",
    "            'has_or_equals': int('OR' in query_str and '=' in query_str),\n",
    "            'has_sleep': int('SLEEP' in query_str or 'WAITFOR' in query_str),\n",
    "            'has_comments': int('--' in query_str or '/*' in query_str or '#' in query_str),\n",
    "            'url_encoded': int('%' in query_str),\n",
    "            'has_info_schema': int('INFORMATION_SCHEMA' in query_str),\n",
    "            'has_exec': int('EXEC' in query_str or 'EXECUTE' in query_str),\n",
    "        }\n",
    "        \n",
    "        features = pd.concat([features, pd.DataFrame([feature_dict])], ignore_index=True)\n",
    "    \n",
    "    return features\n",
    "\n",
    "print(\"Extracting features...\")\n",
    "print(\"This may take a few minutes for large datasets...\\n\")\n",
    "\n",
    "# Extract numeric features\n",
    "print(\"1. Extracting numeric features...\")\n",
    "numeric_features_train = extract_numeric_features(X_train)\n",
    "numeric_features_val = extract_numeric_features(X_val)\n",
    "numeric_features_test = extract_numeric_features(X_test)\n",
    "print(f\"   âœ“ Numeric features shape: {numeric_features_train.shape}\")\n",
    "\n",
    "# Fit TF-IDF vectorizer on training data\n",
    "print(\"\\n2. Fitting TF-IDF vectorizer...\")\n",
    "tfidf = TfidfVectorizer(**CONFIG['tfidf'])\n",
    "tfidf_train = tfidf.fit_transform(X_train)\n",
    "tfidf_val = tfidf.transform(X_val)\n",
    "tfidf_test = tfidf.transform(X_test)\n",
    "print(f\"   âœ“ TF-IDF features shape: {tfidf_train.shape}\")\n",
    "print(f\"   âœ“ Vocabulary size: {len(tfidf.vocabulary_):,}\")\n",
    "\n",
    "# Scale numeric features\n",
    "print(\"\\n3. Scaling numeric features...\")\n",
    "scaler = StandardScaler()\n",
    "numeric_scaled_train = scaler.fit_transform(numeric_features_train)\n",
    "numeric_scaled_val = scaler.transform(numeric_features_val)\n",
    "numeric_scaled_test = scaler.transform(numeric_features_test)\n",
    "\n",
    "# Combine TF-IDF and numeric features\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "print(\"\\n4. Combining features...\")\n",
    "X_train_full = hstack([tfidf_train, csr_matrix(numeric_scaled_train)])\n",
    "X_val_full = hstack([tfidf_val, csr_matrix(numeric_scaled_val)])\n",
    "X_test_full = hstack([tfidf_test, csr_matrix(numeric_scaled_test)])\n",
    "\n",
    "print(f\"\\nâœ“ Final feature matrices:\")\n",
    "print(f\"  - Train: {X_train_full.shape}\")\n",
    "print(f\"  - Val:   {X_val_full.shape}\")\n",
    "print(f\"  - Test:  {X_test_full.shape}\")\n",
    "\n",
    "# Save feature extractors\n",
    "print(\"\\n5. Saving feature extractors...\")\n",
    "joblib.dump(tfidf, ML_MODELS_DIR / 'tfidf_vectorizer.joblib')\n",
    "joblib.dump(scaler, ML_MODELS_DIR / 'numeric_scaler.joblib')\n",
    "print(\"   âœ“ Saved to ml/models/\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
